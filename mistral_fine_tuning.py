# -*- coding: utf-8 -*-
"""Copy of Fixed_Answer_Grading_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWAjbQYEsnauy6v_i9a6GMEb8eo7Cu32
"""

!pip install bitsandbytes
!pip install transformers accelerate peft datasets
!pip install --upgrade bitsandbytes
!pip install --upgrade transformers accelerate peft

pip install pandas datasets openpyxl

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from datasets import Dataset

import pandas as pd
from datasets import Dataset

df = pd.read_json("/content/drive/MyDrive/virtualization_cpu_eval_dataset_simplified.json")

# Convert to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

print(dataset)



!huggingface-cli login

def format_example(example):
    prompt = (
        f"<s>[INST] You are an expert AI grader. Based on the rubric, model answer, and the student's response, do the following:\n"
        f"1. Say if the student answer is correct or not.\n"
        f"2. Justify your evaluation with step-by-step reasoning.\n"
        f"3. Finally, give a numerical score (0â€“5) at the end.\n\n"
        f"Question: {example['question']}\n\n"
        f"Model Answer: {example['ideal_answer']}\n\n"
        f"Student Answer: {example['student_answer']}\n\n"
        f"Rubric: {example['rubric']}\n\n"
        f"Provide your answer below: [/INST] {example['cot_review'].strip()}\nScore: {str(example['score'])} </s>"
    )
    return {"text": prompt}

# Apply formatting and split dataset
dataset = dataset.map(format_example)
dataset = dataset.train_test_split(test_size=0.1, seed=42)

from transformers import AutoTokenizer

model_name = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # For padding in causal LM

def tokenize(batch):
    tokens = tokenizer(
        batch["text"],  # âœ… Correct column name
        padding="max_length",
        truncation=True,
        max_length=1024,
    )
    tokens["labels"] = tokens["input_ids"].copy()  # for CausalLM training
    return tokens

tokenized_dataset = dataset.map(
    tokenize,
    batched=True,
    remove_columns=dataset["train"].column_names
)



!pip install --upgrade bitsandbytes
!pip install --upgrade transformers accelerate peft

from transformers import AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # or "fp4" if supported
    bnb_4bit_compute_dtype="float16",
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

# Ensure `model` is already loaded using a quantized configuration (e.g., using `bitsandbytes` 4-bit/8-bit)
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,                          # Rank of LoRA layers
    lora_alpha=32,                # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Target modules to inject LoRA into (depends on model arch)
    lora_dropout=0.05,            # Dropout regularization
    bias="none",                  # Don't train bias
    task_type="CAUSAL_LM"         # Task type
)

model = get_peft_model(model, lora_config)

!pip install -U transformers

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    num_train_epochs=3,
    learning_rate=5e-5,
    fp16=True,  # Set to False if you're on CPU or Apple Silicon unless using AMP with MPS
    logging_dir='./logs',
    logging_steps=10,
    save_total_limit=2,
    save_steps=100,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

trainer.train()

trainer.save_model("/content/drive/MyDrive/fine-tuned-mistral1")

from peft import PeftModel, PeftConfig
from transformers import AutoTokenizer, AutoModelForCausalLM

peft_model_path = "/content/drive/MyDrive/fine-tuned-mistral1"

# Load the PEFT config (gets base model name)
config = PeftConfig.from_pretrained(peft_model_path)

from peft import PeftModel, PeftConfig
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

peft_model_path = "/content/drive/MyDrive/fine-tuned-mistral1"
config = PeftConfig.from_pretrained(peft_model_path)

# Use 4-bit loading with offload to CPU allowed
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    llm_int8_enable_fp32_cpu_offload=True  # ðŸ‘ˆ this is the key fix
)

# Explicit device map fallback to auto (some will go to CPU)
base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    quantization_config=bnb_config,
    device_map="auto",  # Let it offload parts if needed
    trust_remote_code=True
)

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, peft_model_path)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
